# ğŸ¤– AI Discussion MCP Server

Optimized MCP server for multi-model AI discussions with intelligent context management and token optimization.

## âœ¨ Features

- **Multi-Model Support**: Claude 4.0 (Augment internal), Gemini 2.5 Pro/Flash, DeepSeek R1
- **Context Optimization**: Intelligent token management and compression
- **Cost Efficient**: Claude 4.0 free via Augment, optimized pricing for other models
- **Parallel Processing**: Simultaneous model calls for faster discussions
- **Smart Context Sharing**: All models receive the same conversation context

## ğŸš€ Quick Setup

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure API Keys
Create or update `augment_config.json`:
```json
{
  "openrouter_api_key": "your_openrouter_key_for_deepseek_r1",
  "gemini_api_key": "your_google_api_key_for_gemini"
}
```

### 3. Add to Augment
Add to your Augment MCP configuration:
```json
{
  "mcpServers": {
    "ai-discussion": {
      "command": "python3",
      "args": ["mcp_discussion_server.py"],
      "cwd": "/path/to/mcp_sdk",
      "env": {
        "OPENROUTER_API_KEY": "your_openrouter_api_key",
        "GEMINI_API_KEY": "your_gemini_api_key"
      }
    }
  }
}
```

### 4. Usage in Augment
```
Tháº£o luáº­n giá»¯a Claude, Gemini vÃ  DeepSeek vá» cÃ¡ch tá»‘i Æ°u database performance
```

## ğŸ› ï¸ Available Tools

### Core Discussion Tools

### 1. `discuss` - Multi-Model Discussion
**Purpose:** Detailed multi-perspective analysis vá»›i synthesis
**Usage:** "HÃ£y tháº£o luáº­n vá» [topic] giá»¯a r1 vÃ  gemini"

**Examples:**
- "Tháº£o luáº­n vá» microservices vs monolith cho startup"
- "So sÃ¡nh Python vs Go cho backend API"
- "Discuss pros and cons of TypeScript"

### 2. `chat` - Simple Chat
**Purpose:** Direct conversation vá»›i specific model
**Usage:** "Chat vá»›i [model] vá» [topic]"

**Examples:**
- "Chat vá»›i deepseek vá» async programming"
- "Há»i r1 vá» best practices cho REST API"
- "Ask AI about JWT implementation"

### 3. `consensus` - Multi-Model Consensus
**Purpose:** Focused opinion synthesis vá»›i clear recommendation
**Usage:** "Láº¥y consensus vá» [question]"

**Examples:**
- "Consensus vá» database choice cho e-commerce platform"
- "Ã kiáº¿n chung vá» cloud providers (AWS vs GCP vs Azure)"
- "Láº¥y consensus vá» programming language cho AI project"

**ğŸ“Š Tool Comparison:**
- **`discuss`** = Detailed exploration ğŸ”
- **`chat`** = Quick answers âš¡
- **`consensus`** = Clear decisions ğŸ¯

## API Reference

### `configure(api_keys: dict)`

Configure API keys for the providers.

**Parameters:**
- `api_keys`: Dictionary with provider names as keys and API keys as values
  - Supported providers: `'openai'`, `'gemini'`, `'openrouter'`

### `chat(prompt: str, model: str, history: list = None)`

Start or continue a conversation.

**Parameters:**
- `prompt`: The user's message
- `model`: Model name to use (e.g., `'gpt-4o-mini'`, `'gemini-1.5-flash'`)
- `history`: Previous conversation history (optional)

**Returns:**
- `tuple`: `(response, new_history)` where response is the AI's reply and new_history contains the updated conversation history

### `get_consensus(prompt: str, models: list[str])`

Get consensus from multiple models.

**Parameters:**
- `prompt`: The question or topic to get consensus on
- `models`: List of model names to consult

**Returns:**
- `str`: Synthesized consensus response from all models

## ğŸ¯ Supported Models

### Claude (Augment Internal - FREE)
- `claude-4` - Claude 4.0 via Augment internal (no API costs)
- `claude` - General Claude reference

### Google Gemini
- `gemini-2.5-pro` - 2M context window, best quality
- `gemini-2.5-flash` - 1M context window, fast & cost-effective
- `gemini-2.5-flash-preview-04-17` - Preview version
- `gemini-2.5-flash-lite-preview-06-17` - Lightweight version

### OpenRouter
- `deepseek-r1` - Cost-effective reasoning model
- `deepseek/deepseek-r1` - Full OpenRouter format

## ğŸ’° Cost Optimization

Optimized for minimal costs:
- **Claude 4.0**: $0.00 (free via Augment internal)
- **Gemini 2.5 Flash**: $0.0008 per 6.5K tokens
- **DeepSeek R1**: $0.0112 per 6.5K tokens

## ğŸš€ Performance Features

- **Context Compression**: 31% reduction with quality preservation
- **Token Caching**: Faster estimation with LRU cache
- **Parallel Processing**: Multiple model calls simultaneously
- **Smart Budget Management**: Intelligent token allocation

## ğŸ“ Project Structure

```
mcp_sdk/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ main_logic.py           # Core chat and consensus functions
â”‚   â””â”€â”€ provider_manager.py     # Provider management and routing
â”œâ”€â”€ providers/                  # AI model providers (Google, OpenRouter)
â”œâ”€â”€ system_prompts/             # Optimized system prompts
â””â”€â”€ utils/
    â”œâ”€â”€ token_utils.py          # Model-specific token estimation
    â”œâ”€â”€ context_compression.py  # Intelligent text compression
    â”œâ”€â”€ model_optimizer.py      # Per-model optimizations
    â”œâ”€â”€ token_budget.py         # Budget management
    â””â”€â”€ conversation_memory.py  # Context persistence
```

## ğŸ“š Documentation

- `OPTIMIZATION_SUMMARY.md` - Detailed optimization guide and results
- `README.md` - This file

## ğŸ“„ License

MIT License
